{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97bd7f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize \n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer \n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tag import pos_tag\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "484583ad",
   "metadata": {},
   "source": [
    "## Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a5b79faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loadDataset(data_dir):\n",
    "    data = {}\n",
    "    for partion in [\"train\",\"test\"]:\n",
    "        data[partion] = []\n",
    "        for sentiment in [\"neg\",\"pos\"]:\n",
    "            label = 1 if sentiment == 'pos' else 0\n",
    "            path = os.path.join(data_dir,partion,sentiment)\n",
    "            files = os.listdir(path)\n",
    "            for f_name in files:\n",
    "                with open(os.path.join(path,f_name),'r',encoding='gbk',errors='ignore') as f:\n",
    "                    review = f.read()\n",
    "                    data[partion].append([review,label])\n",
    "                    \n",
    "    # for comparision\n",
    "    random.seed(11) \n",
    "    random.shuffle(data['train'])\n",
    "    random.shuffle(data['test'])\n",
    "\n",
    "    data['train'] = pd.DataFrame(data['train'],columns=['text','sentiment'])\n",
    "    data['test'] = pd.DataFrame(data['test'],columns=['text','sentiment'])\n",
    "    return data['train'],data['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "05f140f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                    text  sentiment\n",
      "0      Yesterday my Spanish / Catalan wife and myself...          1\n",
      "1      The 60s (1999) D: Mark Piznarski. Josh Hamilto...          0\n",
      "2      HUSBANDS BEWARE is a remake of the Shemp class...          1\n",
      "3      No plot, crappy acting, and pointless gore.......          0\n",
      "4      Recap: Doctor Markov has developed a new theor...          0\n",
      "...                                                  ...        ...\n",
      "24995  Grand epic as it is, Kenneth Branagh's monumen...          1\n",
      "24996  Nina Foch insists that \"My Name is Julia Ross\"...          1\n",
      "24997  Although I was in this movie playing the part ...          1\n",
      "24998  It's hard to imagine a director capable of suc...          1\n",
      "24999  `Castle of Blood' (aka `Castle of Terror') is ...          1\n",
      "\n",
      "[25000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "data_dir ='IMDB_data/aclImdb/'\n",
    "train_data,test_data = loadDataset(data_dir)\n",
    "\n",
    "print(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7133fb71",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "630b04d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yesterday spanish catalan wife saw emotional l...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mark piznarski josh hamilton julia stiles jerr...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>husbands beware remake shemp classic brideless...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plot crappy acting pointless gore supposed hor...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recap doctor markov developed new theory produ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>grand epic kenneth branagh monumental renderin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>nina foch insists name julia ross film noir al...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>although movie playing part sheriff hodges sti...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>hard imagine director capable godawful crap no...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>castle blood aka castle terror well crafted su...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment\n",
       "0      yesterday spanish catalan wife saw emotional l...          1\n",
       "1      mark piznarski josh hamilton julia stiles jerr...          0\n",
       "2      husbands beware remake shemp classic brideless...          1\n",
       "3      plot crappy acting pointless gore supposed hor...          0\n",
       "4      recap doctor markov developed new theory produ...          0\n",
       "...                                                  ...        ...\n",
       "24995  grand epic kenneth branagh monumental renderin...          1\n",
       "24996  nina foch insists name julia ross film noir al...          1\n",
       "24997  although movie playing part sheriff hodges sti...          1\n",
       "24998  hard imagine director capable godawful crap no...          1\n",
       "24999  castle blood aka castle terror well crafted su...          1\n",
       "\n",
       "[25000 rows x 2 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "\n",
    "# 下载停用词列表\n",
    "import nltk\n",
    "\n",
    "def clean_text(text):\n",
    "    # 去除HTML标签\n",
    "    clean_html = re.sub(r'<.*?>', '', text)\n",
    "    \n",
    "    # 去除非字母字符和数字，并转换为小写\n",
    "    clean_text = re.sub(r'[^a-zA-Z0-9]', ' ', clean_html).lower()\n",
    "    \n",
    "    # 去除标点符号\n",
    "    clean_text = clean_text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # 分词\n",
    "    tokens = word_tokenize(clean_text)\n",
    "    \n",
    "    # 去除包含数字的标记\n",
    "    tokens = [token for token in tokens if not any(c.isdigit() for c in token)]\n",
    "    \n",
    "    # 去除停用词\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # 将分词后的结果用空格连接\n",
    "    cleaned_text = ' '.join(filtered_tokens)\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "train_data['text'] = train_data['text'].apply(clean_text)\n",
    "\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ca761c",
   "metadata": {},
   "source": [
    "## Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "769a3889",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  yesterday spanish catalan wife saw emotional l...   \n",
      "1  mark piznarski josh hamilton julia stiles jerr...   \n",
      "2  husbands beware remake shemp classic brideless...   \n",
      "3  plot crappy acting pointless gore supposed hor...   \n",
      "4  recap doctor markov developed new theory produ...   \n",
      "\n",
      "                                              tokens  \n",
      "0  [yesterday, spanish, catalan, wife, saw, emoti...  \n",
      "1  [mark, piznarski, josh, hamilton, julia, stile...  \n",
      "2  [husbands, beware, remake, shemp, classic, bri...  \n",
      "3  [plot, crappy, acting, pointless, gore, suppos...  \n",
      "4  [recap, doctor, markov, developed, new, theory...  \n"
     ]
    }
   ],
   "source": [
    "# 创建一个新的列，用来存储标记\n",
    "train_data['tokens'] = train_data['text'].apply(lambda x: x.split())  # 假设文本已经被分割成标记\n",
    "\n",
    "print(train_data[['text', 'tokens']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9360cc",
   "metadata": {},
   "source": [
    "## Word Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4940ea00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  yesterday spanish catalan wife saw emotional l...   \n",
      "1  mark piznarski josh hamilton julia stiles jerr...   \n",
      "2  husbands beware remake shemp classic brideless...   \n",
      "3  plot crappy acting pointless gore supposed hor...   \n",
      "4  recap doctor markov developed new theory produ...   \n",
      "\n",
      "                                              tokens  \\\n",
      "0  [yesterday, spanish, catalan, wife, saw, emoti...   \n",
      "1  [mark, piznarski, josh, hamilton, julia, stile...   \n",
      "2  [husbands, beware, remake, shemp, classic, bri...   \n",
      "3  [plot, crappy, acting, pointless, gore, suppos...   \n",
      "4  [recap, doctor, markov, developed, new, theory...   \n",
      "\n",
      "                                     word_embeddings  \n",
      "0  [[0.0062, 0.0206, 0.0599, 0.0042, -0.0157, -0....  \n",
      "1  [[-0.1152, -0.0489, 0.149, -0.1368, -0.0333, 0...  \n",
      "2  [[0.1553, -0.2057, -0.136, 0.0157, 0.0691, 0.0...  \n",
      "3  [[0.0613, -0.0496, -0.106, -0.0673, 0.0807, -0...  \n",
      "4  [[-0.1088, 0.0718, 0.0141, 0.0826, 0.1647, -0....  \n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 加载FastText预训练的词嵌入模型\n",
    "word2vec_model = KeyedVectors.load_word2vec_format('/Users/lemon/Desktop/IMDB_sentiment_analysis/word2Vec/wiki-news-300d-1M.vec', binary=False)\n",
    "\n",
    "\n",
    "# 假设 train_data 中有 'tokens' 列，存储了标记化的文本\n",
    "# 并且已经进行了分词操作，得到了标记列表\n",
    "\n",
    "# 将标记列表中的每个标记转换为词嵌入向量\n",
    "word_embeddings = []\n",
    "for tokens in train_data['tokens']:\n",
    "    embeddings = [word2vec_model[token] if token in word2vec_model else None for token in tokens]\n",
    "    word_embeddings.append(embeddings)\n",
    "\n",
    "# 创建一个新的列来存储词嵌入向量\n",
    "train_data['word_embeddings'] = word_embeddings\n",
    "\n",
    "# 打印展示部分数据，包括原始文本、标记和词嵌入向量\n",
    "print(train_data[['text', 'tokens', 'word_embeddings']].head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21961bcf",
   "metadata": {},
   "source": [
    "## Simple LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eab2f591",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>tokens</th>\n",
       "      <th>word_embeddings</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yesterday spanish catalan wife saw emotional l...</td>\n",
       "      <td>1</td>\n",
       "      <td>[yesterday, spanish, catalan, wife, saw, emoti...</td>\n",
       "      <td>[[0.0062, 0.0206, 0.0599, 0.0042, -0.0157, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mark piznarski josh hamilton julia stiles jerr...</td>\n",
       "      <td>0</td>\n",
       "      <td>[mark, piznarski, josh, hamilton, julia, stile...</td>\n",
       "      <td>[[-0.1152, -0.0489, 0.149, -0.1368, -0.0333, 0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>husbands beware remake shemp classic brideless...</td>\n",
       "      <td>1</td>\n",
       "      <td>[husbands, beware, remake, shemp, classic, bri...</td>\n",
       "      <td>[[0.1553, -0.2057, -0.136, 0.0157, 0.0691, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>plot crappy acting pointless gore supposed hor...</td>\n",
       "      <td>0</td>\n",
       "      <td>[plot, crappy, acting, pointless, gore, suppos...</td>\n",
       "      <td>[[0.0613, -0.0496, -0.106, -0.0673, 0.0807, -0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>recap doctor markov developed new theory produ...</td>\n",
       "      <td>0</td>\n",
       "      <td>[recap, doctor, markov, developed, new, theory...</td>\n",
       "      <td>[[-0.1088, 0.0718, 0.0141, 0.0826, 0.1647, -0....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24995</th>\n",
       "      <td>grand epic kenneth branagh monumental renderin...</td>\n",
       "      <td>1</td>\n",
       "      <td>[grand, epic, kenneth, branagh, monumental, re...</td>\n",
       "      <td>[[-0.0236, -0.0583, -0.0637, 0.0067, 0.1177, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24996</th>\n",
       "      <td>nina foch insists name julia ross film noir al...</td>\n",
       "      <td>1</td>\n",
       "      <td>[nina, foch, insists, name, julia, ross, film,...</td>\n",
       "      <td>[[-0.0362, 0.0999, 0.0323, -0.126, -0.033, 0.0...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24997</th>\n",
       "      <td>although movie playing part sheriff hodges sti...</td>\n",
       "      <td>1</td>\n",
       "      <td>[although, movie, playing, part, sheriff, hodg...</td>\n",
       "      <td>[[0.0916, -0.0309, -0.0174, 0.1009, -0.1478, -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24998</th>\n",
       "      <td>hard imagine director capable godawful crap no...</td>\n",
       "      <td>1</td>\n",
       "      <td>[hard, imagine, director, capable, godawful, c...</td>\n",
       "      <td>[[-0.1645, -0.1101, -0.3306, 0.0497, -0.0742, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24999</th>\n",
       "      <td>castle blood aka castle terror well crafted su...</td>\n",
       "      <td>1</td>\n",
       "      <td>[castle, blood, aka, castle, terror, well, cra...</td>\n",
       "      <td>[[0.0895, 0.0497, -0.0761, -0.0421, 0.0201, -0...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>25000 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    text  sentiment  \\\n",
       "0      yesterday spanish catalan wife saw emotional l...          1   \n",
       "1      mark piznarski josh hamilton julia stiles jerr...          0   \n",
       "2      husbands beware remake shemp classic brideless...          1   \n",
       "3      plot crappy acting pointless gore supposed hor...          0   \n",
       "4      recap doctor markov developed new theory produ...          0   \n",
       "...                                                  ...        ...   \n",
       "24995  grand epic kenneth branagh monumental renderin...          1   \n",
       "24996  nina foch insists name julia ross film noir al...          1   \n",
       "24997  although movie playing part sheriff hodges sti...          1   \n",
       "24998  hard imagine director capable godawful crap no...          1   \n",
       "24999  castle blood aka castle terror well crafted su...          1   \n",
       "\n",
       "                                                  tokens  \\\n",
       "0      [yesterday, spanish, catalan, wife, saw, emoti...   \n",
       "1      [mark, piznarski, josh, hamilton, julia, stile...   \n",
       "2      [husbands, beware, remake, shemp, classic, bri...   \n",
       "3      [plot, crappy, acting, pointless, gore, suppos...   \n",
       "4      [recap, doctor, markov, developed, new, theory...   \n",
       "...                                                  ...   \n",
       "24995  [grand, epic, kenneth, branagh, monumental, re...   \n",
       "24996  [nina, foch, insists, name, julia, ross, film,...   \n",
       "24997  [although, movie, playing, part, sheriff, hodg...   \n",
       "24998  [hard, imagine, director, capable, godawful, c...   \n",
       "24999  [castle, blood, aka, castle, terror, well, cra...   \n",
       "\n",
       "                                         word_embeddings  \n",
       "0      [[0.0062, 0.0206, 0.0599, 0.0042, -0.0157, -0....  \n",
       "1      [[-0.1152, -0.0489, 0.149, -0.1368, -0.0333, 0...  \n",
       "2      [[0.1553, -0.2057, -0.136, 0.0157, 0.0691, 0.0...  \n",
       "3      [[0.0613, -0.0496, -0.106, -0.0673, 0.0807, -0...  \n",
       "4      [[-0.1088, 0.0718, 0.0141, 0.0826, 0.1647, -0....  \n",
       "...                                                  ...  \n",
       "24995  [[-0.0236, -0.0583, -0.0637, 0.0067, 0.1177, -...  \n",
       "24996  [[-0.0362, 0.0999, 0.0323, -0.126, -0.033, 0.0...  \n",
       "24997  [[0.0916, -0.0309, -0.0174, 0.1009, -0.1478, -...  \n",
       "24998  [[-0.1645, -0.1101, -0.3306, 0.0497, -0.0742, ...  \n",
       "24999  [[0.0895, 0.0497, -0.0761, -0.0421, 0.0201, -0...  \n",
       "\n",
       "[25000 rows x 4 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3b465a82",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda\\envs\\studypytorch\\lib\\site-packages\\ipykernel_launcher.py:12: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "setting an array element with a sequence.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;31mTypeError\u001b[0m: only size-1 arrays can be converted to Python scalars",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-52-1cbd60ad22d4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# 标准化序列长度并处理空向量\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mmax_sequence_length\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m100\u001b[0m  \u001b[1;31m# 设置一个适当的序列最大长度\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mX_train_padded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mX_test_padded\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_sequence_length\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'float32'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtruncating\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'post'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\studypytorch\\lib\\site-packages\\keras\\preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m    152\u001b[0m   return sequence.pad_sequences(\n\u001b[0;32m    153\u001b[0m       \u001b[0msequences\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmaxlen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 154\u001b[1;33m       padding=padding, truncating=truncating, value=value)\n\u001b[0m\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m keras_export(\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\studypytorch\\lib\\site-packages\\keras_preprocessing\\sequence.py\u001b[0m in \u001b[0;36mpad_sequences\u001b[1;34m(sequences, maxlen, dtype, padding, truncating, value)\u001b[0m\n\u001b[0;32m     96\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m         \u001b[1;31m# check `trunc` has expected shape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 98\u001b[1;33m         \u001b[0mtrunc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     99\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtrunc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0msample_shape\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m             raise ValueError('Shape of sample %s of sequence at position %s '\n",
      "\u001b[1;32mD:\\Anaconda\\envs\\studypytorch\\lib\\site-packages\\numpy\\core\\_asarray.py\u001b[0m in \u001b[0;36masarray\u001b[1;34m(a, dtype, order)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     82\u001b[0m     \"\"\"\n\u001b[1;32m---> 83\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0morder\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0morder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     84\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: setting an array element with a sequence."
     ]
    }
   ],
   "source": [
    "# 准备训练数据\n",
    "X = train_data['word_embeddings'].tolist()  # 将词嵌入列表转换为二维列表\n",
    "y = train_data['sentiment'].values\n",
    "\n",
    "# 数据集划分\n",
    "train_size = int(0.8 * len(X))\n",
    "X_train, X_val = X[:train_size], X[train_size:]\n",
    "y_train, y_val = y[:train_size], y[train_size:]\n",
    "\n",
    "# 定义LSTM模型参数\n",
    "input_dim = X_train[0][0].shape[0]  # 词嵌入维度\n",
    "hidden_dim = 128  # LSTM隐藏层维度\n",
    "output_dim = 1  # 输出维度\n",
    "\n",
    "# 初始化权重和偏置\n",
    "weights = {\n",
    "    'input_to_hidden': np.random.randn(input_dim, hidden_dim),\n",
    "    'hidden_to_output': np.random.randn(hidden_dim, output_dim),\n",
    "}\n",
    "bias = {\n",
    "    'hidden': np.zeros(hidden_dim),\n",
    "    'output': np.zeros(output_dim),\n",
    "}\n",
    "\n",
    "# 激活函数\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "# 训练LSTM模型\n",
    "\n",
    "# LSTM模型参数\n",
    "learning_rate = 0.001\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in range(len(X_train)):\n",
    "        seq_len = len(X_train[i])\n",
    "        hidden_states = np.zeros((seq_len, hidden_dim))\n",
    "        hidden_activations = np.zeros((seq_len, hidden_dim))\n",
    "        \n",
    "        # 填充 None 的词嵌入向量为零向量\n",
    "        filled_embeddings = [embedding if embedding is not None else np.zeros(input_dim) for embedding in X_train[i]]\n",
    "        \n",
    "        # 前向传播\n",
    "        for t in range(seq_len):\n",
    "            hidden_states[t] = np.dot(filled_embeddings[t], weights['input_to_hidden']) + bias['hidden']\n",
    "            hidden_activations[t] = sigmoid(hidden_states[t])\n",
    "            \n",
    "        output = np.dot(hidden_activations[-1], weights['hidden_to_output']) + bias['output']\n",
    "        output_activation = sigmoid(output)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = (output_activation - y_train[i]) ** 2\n",
    "        total_loss += loss\n",
    "        \n",
    "        # 反向传播\n",
    "        output_error = output_activation - y_train[i]\n",
    "        output_delta = output_error * output_activation * (1 - output_activation)\n",
    "        \n",
    "        hidden_error = np.dot(output_delta, weights['hidden_to_output'].T)\n",
    "        hidden_delta = hidden_error * hidden_activations[-1] * (1 - hidden_activations[-1])\n",
    "        \n",
    "        # 更新权重和偏置\n",
    "        weights['hidden_to_output'] -= learning_rate * np.outer(hidden_activations[-1], output_delta)\n",
    "        \n",
    "        for t in reversed(range(seq_len)):\n",
    "            if t > 0:\n",
    "                hidden_grad = hidden_delta * hidden_activations[t] * (1 - hidden_activations[t])\n",
    "                weights['input_to_hidden'] -= learning_rate * np.outer(filled_embeddings[t], hidden_grad)\n",
    "                hidden_delta = np.dot(weights['input_to_hidden'],hidden_grad) * hidden_delta * (1 - hidden_activations[t-1]**2)\n",
    "            else:\n",
    "                weights['input_to_hidden'] -= learning_rate * np.outer(filled_embeddings[t], hidden_delta)\n",
    "        \n",
    "        bias['output'] -= learning_rate * output_delta\n",
    "        bias['hidden'] -= learning_rate * hidden_delta\n",
    "    \n",
    "    avg_loss = total_loss / len(X_train)\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Average Loss: {avg_loss:.4f}\")\n",
    "\n",
    "print(\"Training finished.\")\n",
    "\n",
    "\n",
    "# 预测与评估\n",
    "val_loss = 0\n",
    "correct_predictions = 0\n",
    "\n",
    "for i in range(len(X_val)):\n",
    "    seq_len = len(X_val[i])\n",
    "    hidden_states_val = np.zeros((seq_len, hidden_dim))\n",
    "    hidden_activations_val = np.zeros((seq_len, hidden_dim))\n",
    "    \n",
    "    # 填充 None 的词嵌入向量为零向量\n",
    "    filled_embeddings_val = [embedding if embedding is not None else np.zeros(input_dim) for embedding in X_val[i]]\n",
    "    \n",
    "    for t in range(seq_len):\n",
    "        hidden_states_val[t] = np.dot(filled_embeddings_val[t], weights['input_to_hidden']) + bias['hidden']\n",
    "        hidden_activations_val[t] = sigmoid(hidden_states_val[t])\n",
    "        \n",
    "    output_val = np.dot(hidden_activations_val[-1], weights['hidden_to_output']) + bias['output']\n",
    "    output_activation_val = sigmoid(output_val)\n",
    "\n",
    "    val_loss += (output_activation_val - y_val[i]) ** 2\n",
    "    prediction = 1 if output_activation_val > 0.5 else 0\n",
    "    if prediction == y_val[i]:\n",
    "        correct_predictions += 1\n",
    "\n",
    "avg_val_loss = val_loss / len(X_val)\n",
    "accuracy = correct_predictions / len(X_val)\n",
    "\n",
    "print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "print(f\"Accuracy: {accuracy:.2%}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
